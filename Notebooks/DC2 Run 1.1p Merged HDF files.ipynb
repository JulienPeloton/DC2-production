{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at merged tract-patch catalogs in Run 1.1 leading up to DC2\n",
    "Michael Wood-Vasey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tract_data_dir = '/global/projecta/projectdirs/lsst/global/in2p3/Run1.1-test2/summary/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract, patch = 4848, 16\n",
    "basename = 'merged_tract_%d.hdf5' % tract\n",
    "key = 'coadd_%d_%2d' % (tract, patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = 'merged_tract_4849_1,1_1,2.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tract_file = os.path.join(merged_tract_data_dir, basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(merged_tract_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df), \"coadd objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['g_mag'] - df['r_mag'], df['r_mag'] - df['i_mag'], marker='.')\n",
    "plt.xlim(-1, +2)\n",
    "plt.ylim(-1, +2)\n",
    "plt.xlabel('g-r')\n",
    "plt.ylabel('r-i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit of a mess, but maybe we're starting to see a bit of some structure resolving.  Let's do the traditional simple 2D histogram version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist2d(df['g_mag']-df['r_mag'], df['r_mag']-df['i_mag'],\n",
    "           range=((-1, +2), (-1, +2)), bins=51)\n",
    "plt.colorbar()\n",
    "plt.xlabel('g-r')\n",
    "plt.ylabel('r-i')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, that looks familiar!  I feel like I've taken some class where I saw something like that.\n",
    "\n",
    "Hmmm... I wonder which of these are stars and which galaxies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_max_extended = 1.0\n",
    "stars = df[df.base_ClassificationExtendedness_value < safe_max_extended]\n",
    "galaxies = df[df.base_ClassificationExtendedness_value >= safe_max_extended]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist_kwargs = {'bins': np.linspace(-2, 3, 51),\n",
    "               'range': (-2, +3),\n",
    "               'linewidth': 4,\n",
    "               'histtype': 'step'}\n",
    "          \n",
    "plt.hist(df['g_mag'] - df['r_mag'], label='all', **hist_kwargs)\n",
    "plt.hist(galaxies['g_mag'] - galaxies['r_mag'], label='galaxies', **hist_kwargs)\n",
    "plt.hist(stars['g_mag'] - stars['r_mag'], label='stars', **hist_kwargs)\n",
    "plt.xlabel('g-r')\n",
    "plt.ylabel('# / 0.5 mag bin')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z = galaxies\n",
    "zi, xedges, yedges = np.histogram2d(z['g_mag']-z['r_mag'], z['r_mag']-z['i_mag'],\n",
    "                                    range=((-1, +2), (-1, +2)), bins=31)\n",
    "xi = (xedges[1:] + xedges[:-1])/2\n",
    "yi = (yedges[1:] + yedges[:-1])/2\n",
    "\n",
    "\n",
    "plt.pcolormesh(xi, yi, zi, cmap='Oranges')\n",
    "plt.contour(xi, yi, zi)\n",
    "plt.xlabel('g-r')\n",
    "plt.ylabel('r-i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z = stars\n",
    "zi, xedges, yedges = np.histogram2d(z['g_mag']-z['r_mag'], z['r_mag']-z['i_mag'],\n",
    "                                    range=((-1, +2), (-1, +2)), bins=31)\n",
    "xi = (xedges[1:] + xedges[:-1])/2\n",
    "yi = (yedges[1:] + yedges[:-1])/2\n",
    "\n",
    "plt.pcolormesh(xi, yi, zi, cmap='Oranges')\n",
    "plt.contour(xi, yi, zi)\n",
    "plt.xlabel('g-r')\n",
    "plt.ylabel('r-i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation\n",
    "We could entertain ourselves by doing a somewhat better density estimattion\n",
    "E.g., a Gaussian kernel density estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19390320/scatterplot-contours-in-matplotlib\n",
    "def bin_kde(x, y, nbins=20, **kwargs):\n",
    "    from scipy.stats import kde\n",
    "    k = kde.gaussian_kde(x, y)\n",
    "    xi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbis*1j]\n",
    "    zi = k(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "\n",
    "# This doesn't exactly work, I've done something wrong here.\n",
    "xi, yi, zi = bin_kde(stars['g_mag']-stars['r_mag'], stars['r_mag']-stars['i_mag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Catalog Reader (GCR)\n",
    "\n",
    "After a little bit you'll find yourself wanting to ask interesting questions, like how well does the DC2 analysis recover the input sources?  A convenient framework for generalizing analyses of DESC-DC related catalogs is the the Generic Catalog Reader:\n",
    "\n",
    "* GCR is a general utility\n",
    "https://github.com/yymao/generic-catalog-reader\n",
    "* The catalogs supporting DC2 are in:\n",
    "https://github.com/LSSTDESC/gcr-catalogs\n",
    "\n",
    "See 'DC2_Static_Coadd_CAtalog_reader.ipynb' notebook by Yao-Yuan Mao\n",
    "\n",
    "https://github.com/LSSTDESC/DC2_Repo/blob/u/yymao/GCRforDC2Coadd/Notebooks/DC2%20Static%20Coadd%20Catalog%20reader.ipynb\n",
    "\n",
    "We don't have a package management set up for our DESC+DC2 products, so that notebook starts with a bit of explicit insertion of DC2 scripts into the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "debug180313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
