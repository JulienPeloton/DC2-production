{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Tests of Apache Spark-Based DC2 Run 1.1 Object Catalog Access\n",
    "Author: **Julien Peloton [@JulienPeloton](https://github.com/JulienPeloton)**  \n",
    "Last Run: **2018-10-15**  \n",
    "See also: [issue/249](https://github.com/LSSTDESC/DC2-production/issues/249)\n",
    "\n",
    "Introduce Apache Spark and test performance of data manipulations of the static coadd catalogs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting...\n",
    "\n",
    "**What is Apache Spark?**\n",
    "\n",
    "I'm glad you asked! [Apache Spark](http://spark.apache.org/) is a cluster computing framework, that is a set of tools to perform computation on a network of many machines. Spark started in 2009 as a research project, and it had a huge success so far in the industry. It is based on the so-called MapReduce cluster computing paradigm, popularized by the Hadoop framework using implicit data parallelism and fault tolerance. \n",
    "\n",
    "**Where to find information on running Spark at NERSC?**\n",
    "\n",
    "Most of what you need for interactive and batch jobs is at [spark-distributed-analytic-framework](https://www.nersc.gov/users/data-analytics/data-analytics-2/spark-distributed-analytic-framework/).\n",
    "\n",
    "For JupyterLab use, see below.\n",
    "\n",
    "**Where this Notebook is intended to be run?**\n",
    "\n",
    "These tests were conducted on NERSC through the https://jupyter-dev.nersc.gov interface.\n",
    "\n",
    "**What is needed to run this Notebook at NERSC?**\n",
    "\n",
    "1. You need an account at NERSC, and access to the DESC allocation.\n",
    "\n",
    "2. This Notebook requires a pyspark kernel. The easiest way to get one, \n",
    "is to generate one using the [spark-kernel-nersc](https://github.com/astrolabsoftware/spark-kernel-nersc) repository. \n",
    "The simplest kernel to play with this Notebook would be generated using:\n",
    "\n",
    "```bash\n",
    "# Clone the repo\n",
    "git clone https://github.com/astrolabsoftware/spark-kernel-nersc.git\n",
    "cd spark-kernel-nersc\n",
    "\n",
    "# Where the Spark logs will be stored\n",
    "# Logs can be then be browsed from the Spark UI\n",
    "LOGDIR=${SCRATCH}/spark/event_logs\n",
    "mkdir -p ${LOGDIR}\n",
    "\n",
    "# Resource to use. Here we will use 4 CPUs.\n",
    "RESOURCE=local[4]\n",
    "\n",
    "# Extra libraries (comma separated if many) to use.\n",
    "SPARKFITS=com.github.astrolabsoftware:spark-fits_2.11:0.7.0\n",
    "\n",
    "# Create the kernel - will be stored under\n",
    "# \\$HOME/.ipython/kernels/<kernelname>\n",
    "python makekernel.py \\\n",
    "  -kernelname pyspark_2.3.0 -spark_version 2.3.0 \\ \n",
    "  -pyspark_args \"--master ${RESOURCE} \n",
    "  --conf spark.eventLog.enabled=true \n",
    "  --conf spark.eventLog.dir=file://${LOGDIR} \n",
    "  --conf spark.history.fs.logDirectory=file://${LOGDIR} \n",
    "  --packages ${SPARKFITS}\"\n",
    "```\n",
    "\n",
    "Finally log on https://jupyter-dev.nersc.gov, open this Notebook and choose the kernel you just created.\n",
    "If you encounter problem with this kernel (I do have sometimes problems with Shifter), switch to another Spark version not using Shifter: `python makekernel.py -kernelname pyspark_2.1.0 -spark_version 2.1.0 ...`.\n",
    "\n",
    "**Where is the data used in this Notebook?**\n",
    "\n",
    "Data used can be found at \n",
    "\n",
    "```\n",
    "/global/projecta/projectdirs/lsst/global/in2p3/Run1.1/summary/\n",
    "```\n",
    "\n",
    "Apache Spark can read a large number of data formats (Parquet, Avro, text) but officially neither FITS nor HDF5 are supported. We developed a solution for FITS files ([spark-fits](https://github.com/astrolabsoftware/spark-fits), Scala/Java/Python/R API), but as far as I know there is no Python-friendly connector for HDF5. Therefore we will focus only on Parquet and FITS in this Notebook.\n",
    "\n",
    "**Note concerning resources**\n",
    "\n",
    "```\n",
    "The large-memory login node used by https://jupyter-dev.nersc.gov/\n",
    "is a shared resource, so please be careful not to use too many CPUs\n",
    "or too much memory.\n",
    "\n",
    "That means avoid using `--master local[*]` in your kernel, but limit\n",
    "the resources to a few core. Typically `--master local[4]` is enough for\n",
    "prototyping a program.\n",
    "\n",
    "Then to scale the analysis, the best is to switch to batch mode! \n",
    "Here, no limit!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We will follow what is done on the Dask Notebook (put link).\n",
    "We will first focus on one `patch` (4850) with all `tracts`.\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "Apache Spark, is meant to be primarily used in a context of _big data_.\n",
    "One of its strength is its scalability, namely its capability of using the same\n",
    "piece of code regardless the underlying data volume. The performance of the code will\n",
    "then only depend on the resource used. \n",
    "E.g. for tasks without communications, execution time will be linear with data or resource.\n",
    "\n",
    "Keep in mind:\n",
    "- For small volume of data (< 10 GB), you will hit Spark noise and burning time.\n",
    "- Spark is written in Scala, which is certainly not as specialised as C++ could be. Therefore for small volume of data, there is a chance an algorithm in Scala (Spark) would be slower than its C++ counterpart. But the Spark one is meant to run on TB of data _as it was written_ for MB of data - which is probably not the case for the C++.\n",
    "- Once the data loaded, you can decide to keep it in memory (distributed among the executors). The next iterations will then go super fast (typically disk I/O throughput is o(100) MB/s while RAM is o(10) GB/s).\n",
    "\n",
    "So in this example, the one patch test is likely to lessen Spark performance (volume is just few GB here), and these tests must also be ran on hundreds of GB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/global/projecta/projectdirs/lsst/global/in2p3/Run1.1/summary'\n",
    "\n",
    "# Load one patch, all tracts\n",
    "datafile = os.path.join(base_dir, 'dpdd_object.parquet')\n",
    "print(\"Data will be read from: \\n\", datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into a DataFrame\n",
    "\n",
    "Let's initialise Spark and load the data into a DataFrame. We will first focus on the `parquet` data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- magerr_i: double (nullable = true)\n",
      " |-- psFlux_i: double (nullable = true)\n",
      " |-- Ixx_r: double (nullable = true)\n",
      " |-- mag_i_cModel: double (nullable = true)\n",
      " |-- IxxPSF_u: double (nullable = true)\n",
      " |-- magerr_r: double (nullable = true)\n",
      " |-- psf_fwhm_i: double (nullable = true)\n",
      " |-- psf_fwhm_r: double (nullable = true)\n",
      " |-- Ixx: double (nullable = true)\n",
      " |-- magerr_g_cModel: double (nullable = true)\n",
      " |-- I_flag_y: boolean (nullable = true)\n",
      " |-- Iyy_z: double (nullable = true)\n",
      " |-- IxyPSF_i: double (nullable = true)\n",
      " |-- Ixx_z: double (nullable = true)\n",
      " |-- magerr_u_cModel: double (nullable = true)\n",
      " |-- IxyPSF: double (nullable = true)\n",
      " |-- snr_u_cModel: double (nullable = true)\n",
      " |-- IxxPSF_y: double (nullable = true)\n",
      " |-- psFlux_flag_i: boolean (nullable = true)\n",
      " |-- IyyPSF_g: double (nullable = true)\n",
      " |-- Ixy: double (nullable = true)\n",
      " |-- magerr_y: double (nullable = true)\n",
      " |-- psFlux_g: double (nullable = true)\n",
      " |-- snr_y_cModel: double (nullable = true)\n",
      " |-- Ixy_z: double (nullable = true)\n",
      " |-- psFlux_flag_r: boolean (nullable = true)\n",
      " |-- Iyy_g: double (nullable = true)\n",
      " |-- psFluxErr_r: double (nullable = true)\n",
      " |-- Ixx_i: double (nullable = true)\n",
      " |-- snr_z_cModel: double (nullable = true)\n",
      " |-- psf_fwhm_g: double (nullable = true)\n",
      " |-- Ixx_y: double (nullable = true)\n",
      " |-- I_flag: boolean (nullable = true)\n",
      " |-- magerr_z: double (nullable = true)\n",
      " |-- I_flag_i: boolean (nullable = true)\n",
      " |-- IyyPSF_i: double (nullable = true)\n",
      " |-- yErr: float (nullable = true)\n",
      " |-- magerr_r_cModel: double (nullable = true)\n",
      " |-- magerr_i_cModel: double (nullable = true)\n",
      " |-- clean: boolean (nullable = true)\n",
      " |-- IxyPSF_y: double (nullable = true)\n",
      " |-- mag_g: double (nullable = true)\n",
      " |-- mag_r: double (nullable = true)\n",
      " |-- psf_fwhm_y: double (nullable = true)\n",
      " |-- IxyPSF_g: double (nullable = true)\n",
      " |-- ra: double (nullable = true)\n",
      " |-- extendedness: double (nullable = true)\n",
      " |-- IxxPSF: double (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- Ixx_u: double (nullable = true)\n",
      " |-- mag_g_cModel: double (nullable = true)\n",
      " |-- psFluxErr_u: double (nullable = true)\n",
      " |-- I_flag_r: boolean (nullable = true)\n",
      " |-- IyyPSF_r: double (nullable = true)\n",
      " |-- psFluxErr_y: double (nullable = true)\n",
      " |-- psNdata: float (nullable = true)\n",
      " |-- psFlux_y: double (nullable = true)\n",
      " |-- psFlux_u: double (nullable = true)\n",
      " |-- Iyy: double (nullable = true)\n",
      " |-- IxxPSF_r: double (nullable = true)\n",
      " |-- mag_u: double (nullable = true)\n",
      " |-- dec: double (nullable = true)\n",
      " |-- IxyPSF_z: double (nullable = true)\n",
      " |-- mag_y: double (nullable = true)\n",
      " |-- Ixx_g: double (nullable = true)\n",
      " |-- Ixy_y: double (nullable = true)\n",
      " |-- IxxPSF_i: double (nullable = true)\n",
      " |-- blendedness: double (nullable = true)\n",
      " |-- Iyy_y: double (nullable = true)\n",
      " |-- IxxPSF_g: double (nullable = true)\n",
      " |-- psFlux_flag_u: boolean (nullable = true)\n",
      " |-- psFluxErr_g: double (nullable = true)\n",
      " |-- Iyy_r: double (nullable = true)\n",
      " |-- magerr_u: double (nullable = true)\n",
      " |-- I_flag_g: boolean (nullable = true)\n",
      " |-- snr_i_cModel: double (nullable = true)\n",
      " |-- psFluxErr_i: double (nullable = true)\n",
      " |-- IxxPSF_z: double (nullable = true)\n",
      " |-- IyyPSF_y: double (nullable = true)\n",
      " |-- I_flag_z: boolean (nullable = true)\n",
      " |-- snr_r_cModel: double (nullable = true)\n",
      " |-- Ixy_g: double (nullable = true)\n",
      " |-- mag_z: double (nullable = true)\n",
      " |-- IyyPSF_z: double (nullable = true)\n",
      " |-- psFlux_r: double (nullable = true)\n",
      " |-- IxyPSF_r: double (nullable = true)\n",
      " |-- psFlux_flag_g: boolean (nullable = true)\n",
      " |-- Iyy_u: double (nullable = true)\n",
      " |-- psf_fwhm_u: double (nullable = true)\n",
      " |-- objectId: long (nullable = true)\n",
      " |-- magerr_z_cModel: double (nullable = true)\n",
      " |-- snr_g_cModel: double (nullable = true)\n",
      " |-- psFlux_flag_y: boolean (nullable = true)\n",
      " |-- I_flag_u: boolean (nullable = true)\n",
      " |-- Ixy_u: double (nullable = true)\n",
      " |-- mag_i: double (nullable = true)\n",
      " |-- psFluxErr_z: double (nullable = true)\n",
      " |-- good: boolean (nullable = true)\n",
      " |-- Ixy_r: double (nullable = true)\n",
      " |-- parentObjectId: long (nullable = true)\n",
      " |-- Ixy_i: double (nullable = true)\n",
      " |-- IyyPSF: double (nullable = true)\n",
      " |-- xErr: float (nullable = true)\n",
      " |-- mag_u_cModel: double (nullable = true)\n",
      " |-- xy_flag: boolean (nullable = true)\n",
      " |-- IyyPSF_u: double (nullable = true)\n",
      " |-- mag_r_cModel: double (nullable = true)\n",
      " |-- mag_y_cModel: double (nullable = true)\n",
      " |-- magerr_g: double (nullable = true)\n",
      " |-- mag_z_cModel: double (nullable = true)\n",
      " |-- psf_fwhm_z: double (nullable = true)\n",
      " |-- psFlux_flag_z: boolean (nullable = true)\n",
      " |-- IxyPSF_u: double (nullable = true)\n",
      " |-- Iyy_i: double (nullable = true)\n",
      " |-- psFlux_z: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- magerr_y_cModel: double (nullable = true)\n",
      " |-- tract: integer (nullable = true)\n",
      " |-- patch: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise our Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read the data as DataFrame\n",
    "df = spark.read.format(\"parquet\").load(datafile)\n",
    "\n",
    "# Check what we have in the file\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One tract DataFrame has length: 6892380\n",
      "1 loop, best of 3: 1.33 s per loop\n"
     ]
    }
   ],
   "source": [
    "# Get number of elements\n",
    "print(\"One tract DataFrame has length:\", df.count())\n",
    "%timeit c = df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating the data\n",
    "\n",
    "Let's play with the data. We will see how to compute statistics (mean, std, etc...) and ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def stat_one_col(df: DataFrame, colname: str) -> DataFrame:\n",
    "    \"\"\" Return some statistics about one DataFrame Column.\n",
    "    Statistics include: count, mean, stddev, min, max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    colname : str\n",
    "        Name of the Column for which we want the statistics\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    out : DataFrame\n",
    "        DataFrame containing statistics about the Column.\n",
    "    \"\"\"\n",
    "    return df.select(colname).describe()\n",
    "\n",
    "def stat_diff_col(df: DataFrame, colname_1: str, colname_2: str) -> DataFrame:\n",
    "    \"\"\" Return some statistics about the difference of \n",
    "    two DataFrame Columns.\n",
    "    Statistics include: count, mean, stddev, min, max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    colname_1 : str\n",
    "        Name of the first Column\n",
    "    colname_2 : str\n",
    "        Name of the second Column\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    out : DataFrame\n",
    "        DataFrame containing statistics about the Columns difference.\n",
    "    \"\"\"\n",
    "    return df.select(col(colname_1) - col(colname_2)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             mag_g|\n",
      "+-------+------------------+\n",
      "|  count|           5698019|\n",
      "|   mean|25.721391706804777|\n",
      "| stddev|  2.16444185648169|\n",
      "|    min|14.431595512576623|\n",
      "|    max| 52.39393263760668|\n",
      "+-------+------------------+\n",
      "\n",
      "1 loop, best of 3: 1 s per loop\n",
      "+-------+------------------+\n",
      "|summary|   (mag_g - mag_r)|\n",
      "+-------+------------------+\n",
      "|  count|           4172439|\n",
      "|   mean|0.5274674368917402|\n",
      "| stddev|1.6067276263035577|\n",
      "|    min|-28.13517076318348|\n",
      "|    max|33.316740496333026|\n",
      "+-------+------------------+\n",
      "\n",
      "1 loop, best of 3: 1.07 s per loop\n"
     ]
    }
   ],
   "source": [
    "# Get statistics about one column\n",
    "stat_one_col(df, 'mag_g').show()\n",
    "%timeit c = stat_one_col(df, 'mag_g').collect()\n",
    "\n",
    "# Get statistics for the difference of two columns\n",
    "stat_diff_col(df, 'mag_g', 'mag_r').show()\n",
    "%timeit d = stat_diff_col(df, 'mag_g', 'mag_r').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes roughly 1 second to produce statistics on the full dataset (6 million rows, 4.5 GB). This has to be compared to the 52 seconds for Dask to compute the `mean` using the same resource (4 CPU). Note that we didn't explicitly asked to put the data in cache (would be much faster). Note also that the number of elements for each column used to produce statistics varies (`count`). This is due to the fact that `NaN` are discarded.\n",
    "\n",
    "I also ran this test with more resource (i.e. more CPUs), and the execution time gets smaller. Not linearly, as we hit Spark burning time (not enough data), but e.g. a few hundred of millisecond with 32 CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future\n",
    "\n",
    "- FITS?\n",
    "- More data?\n",
    "- More action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col,sum\n",
    "# df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()\n",
    "# import glob\n",
    "# base_dir = 'file:///global/cscratch1/sd/peloton/dpdd/new_fits/dpdd_object_tract_4639_patch_*.fits'\n",
    "# df = spark.read.format(\"fits\").option(\"hdu\", 1).load(base_dir)\n",
    "# df.count()\n",
    "# for file in base_dir:\n",
    "#     df = spark.read.format(\"fits\").option(\"hdu\", 1).load(file)\n",
    "#     print(file, df.count())\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='once')\n",
    "\n",
    "# Put the data in cache for the following\n",
    "# df_cached = df.cache()\n",
    "\n",
    "# # Trigger an action, to actually put data in cache\n",
    "# # (Spark is lazy, and cache is a transformation!)\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings(action='once')\n",
    "#     df_cached.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_2.3.0 (2.3.0)",
   "language": "python",
   "name": "pyspark_2.3.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
