{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert HDF5 merged-tract file to Parquet.\n",
    "\n",
    "Requires `fastparquet` or `pyarrow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import fastparquet as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_table_dir = '/global/projecta/projectdirs/lsst/global/in2p3/Run1.1/object_catalog/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'trim_'\n",
    "tract = 4848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_table_basename = '{}merged_tract_{}'.format(prefix, tract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_filepath = os.path.join(object_table_dir, object_table_basename+'.hdf5')\n",
    "parquet_filepath = os.path.join(object_table_dir, object_table_basename+'.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 8, 8\n",
    "patches = ['{:1d}{:1d}'.format(x, y) for x in range(nx) for y in range(ny)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018-08-21 MWV\n",
    "Trying to append in Parquet is bringing up the same problem that @yymao has been asking about.  If the number of columns was fixed, that would greatly simplify reading by the Generic Catalog Reader.  But because each filter coadd adds an entire set of columns, when processing tract+patches with missing filters, I've just left them out.\n",
    "\n",
    "This might be the push that I need to make the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/projecta/projectdirs/lsst/global/in2p3/Run1.1/object_catalog/trim_merged_tract_4848.parquet\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'i_base_PsfFlux_fluxSigma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/global/common/software/lsst/common/miniconda/current/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'i_base_PsfFlux_fluxSigma'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-ba67e7ae5401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/lib/python3.6/site-packages/fastparquet/writer.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times)\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_scheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'simple'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         write_simple(filename, data, fmd, row_group_offsets,\n\u001b[0;32m--> 848\u001b[0;31m                      compression, open_with, has_nulls, append)\n\u001b[0m\u001b[1;32m    849\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfile_scheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drill'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/lib/python3.6/site-packages/fastparquet/writer.py\u001b[0m in \u001b[0;36mwrite_simple\u001b[0;34m(fn, data, fmd, row_group_offsets, compression, open_with, has_nulls, append)\u001b[0m\n\u001b[1;32m    715\u001b[0m                    else None)\n\u001b[1;32m    716\u001b[0m             rg = make_row_group(f, data[start:end], fmd.schema,\n\u001b[0;32m--> 717\u001b[0;31m                                 compression=compression)\n\u001b[0m\u001b[1;32m    718\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0mfmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_groups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/lib/python3.6/site-packages/fastparquet/writer.py\u001b[0m in \u001b[0;36mmake_row_group\u001b[0;34m(f, data, schema, compression)\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                 \u001b[0mcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             chunk = write_column(f, data[column.name], column,\n\u001b[0m\u001b[1;32m    614\u001b[0m                                  compression=comp)\n\u001b[1;32m    615\u001b[0m             \u001b[0mrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/lsst/common/miniconda/current/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/lsst/common/miniconda/current/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/lsst/common/miniconda/current/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/lsst/common/miniconda/current/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/lsst/common/miniconda/current/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'i_base_PsfFlux_fluxSigma'"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "\n",
    "print(parquet_filepath)\n",
    "\n",
    "append = False  # Create new file the first time around.\n",
    "for patch in patches:\n",
    "    key = 'coadd_{}_{}'.format(tract, patch)\n",
    "    try:\n",
    "        df = pd.read_hdf(hdf5_filepath, key=key)\n",
    "    except KeyError as ke:\n",
    "        if verbose:\n",
    "            print(ke)\n",
    "        continue\n",
    "\n",
    "    fp.write(parquet_filepath, df, append=append)\n",
    "    append = True  # Once it exists, append subsequent writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function rmdir in module posix:\n",
      "\n",
      "rmdir(path, *, dir_fd=None)\n",
      "    Remove a directory.\n",
      "    \n",
      "    If dir_fd is not None, it should be a file descriptor open to a directory,\n",
      "      and path should be relative; path will then be relative to that directory.\n",
      "    dir_fd may not be implemented on your platform.\n",
      "      If it is unavailable, using it will raise a NotImplementedError.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(os.rmdir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = fp.ParquetFile(parquet_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function write in module fastparquet.writer:\n",
      "\n",
      "write(filename, data, row_group_offsets=50000000, compression=None, file_scheme='simple', open_with=<function default_open at 0x2ae847596378>, mkdirs=<function default_mkdirs at 0x2ae8475962f0>, has_nulls=True, write_index=None, partition_on=[], fixed_text=None, append=False, object_encoding='infer', times='int64')\n",
      "    Write Pandas DataFrame to filename as Parquet Format\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filename: string\n",
      "        Parquet collection to write to, either a single file (if file_scheme\n",
      "        is simple) or a directory containing the metadata and data-files.\n",
      "    data: pandas dataframe\n",
      "        The table to write\n",
      "    row_group_offsets: int or list of ints\n",
      "        If int, row-groups will be approximately this many rows, rounded down\n",
      "        to make row groups about the same size; if a list, the explicit index\n",
      "        values to start new row groups.\n",
      "    compression: str, dict\n",
      "        compression to apply to each column, e.g. ``GZIP`` or ``SNAPPY`` or a\n",
      "        ``dict`` like ``{\"col1\": \"SNAPPY\", \"col2\": None}`` to specify per\n",
      "        column compression types.\n",
      "        In both cases, the compressor settings would be the underlying\n",
      "        compressor defaults. To pass arguments to the underlying compressor,\n",
      "        each ``dict`` entry should itself be a dictionary::\n",
      "    \n",
      "            {\n",
      "                col1: {\n",
      "                    \"type\": \"LZ4\",\n",
      "                    \"args\": {\n",
      "                        \"compression_level\": 6,\n",
      "                        \"content_checksum\": True\n",
      "                     }\n",
      "                },\n",
      "                col2: {\n",
      "                    \"type\": \"SNAPPY\",\n",
      "                    \"args\": None\n",
      "                }\n",
      "                \"_default\": {\n",
      "                    \"type\": \"GZIP\",\n",
      "                    \"args\": None\n",
      "                }\n",
      "            }\n",
      "    \n",
      "        where ``\"type\"`` specifies the compression type to use, and ``\"args\"``\n",
      "        specifies a ``dict`` that will be turned into keyword arguments for\n",
      "        the compressor.\n",
      "        If the dictionary contains a \"_default\" entry, this will be used for any\n",
      "        columns not explicitly specified in the dictionary.\n",
      "    file_scheme: 'simple'|'hive'\n",
      "        If simple: all goes in a single file\n",
      "        If hive: each row group is in a separate file, and a separate file\n",
      "        (called \"_metadata\") contains the metadata.\n",
      "    open_with: function\n",
      "        When called with a f(path, mode), returns an open file-like object\n",
      "    mkdirs: function\n",
      "        When called with a path/URL, creates any necessary dictionaries to\n",
      "        make that location writable, e.g., ``os.makedirs``. This is not\n",
      "        necessary if using the simple file scheme\n",
      "    has_nulls: bool, 'infer' or list of strings\n",
      "        Whether columns can have nulls. If a list of strings, those given\n",
      "        columns will be marked as \"optional\" in the metadata, and include\n",
      "        null definition blocks on disk. Some data types (floats and times)\n",
      "        can instead use the sentinel values NaN and NaT, which are not the same\n",
      "        as NULL in parquet, but functionally act the same in many cases,\n",
      "        particularly if converting back to pandas later. A value of 'infer'\n",
      "        will assume nulls for object columns and not otherwise.\n",
      "    write_index: boolean\n",
      "        Whether or not to write the index to a separate column.  By default we\n",
      "        write the index *if* it is not 0, 1, ..., n.\n",
      "    partition_on: list of column names\n",
      "        Passed to groupby in order to split data within each row-group,\n",
      "        producing a structured directory tree. Note: as with pandas, null\n",
      "        values will be dropped. Ignored if file_scheme is simple.\n",
      "    fixed_text: {column: int length} or None\n",
      "        For bytes or str columns, values will be converted\n",
      "        to fixed-length strings of the given length for the given columns\n",
      "        before writing, potentially providing a large speed\n",
      "        boost. The length applies to the binary representation *after*\n",
      "        conversion for utf8, json or bson.\n",
      "    append: bool (False)\n",
      "        If False, construct data-set from scratch; if True, add new row-group(s)\n",
      "        to existing data-set. In the latter case, the data-set must exist,\n",
      "        and the schema must match the input data.\n",
      "    object_encoding: str or {col: type}\n",
      "        For object columns, this gives the data type, so that the values can\n",
      "        be encoded to bytes. Possible values are bytes|utf8|json|bson|bool|int|int32,\n",
      "        where bytes is assumed if not specified (i.e., no conversion). The\n",
      "        special value 'infer' will cause the type to be guessed from the first\n",
      "        ten non-null values.\n",
      "    times: 'int64' (default), or 'int96':\n",
      "        In \"int64\" mode, datetimes are written as 8-byte integers, us\n",
      "        resolution; in \"int96\" mode, they are written as 12-byte blocks, with\n",
      "        the first 8 bytes as ns within the day, the next 4 bytes the julian day.\n",
      "        'int96' mode is included only for compatibility.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> fastparquet.write('myfile.parquet', df)  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fp.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python-dask",
   "language": "python",
   "name": "desc-python-dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
