{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: summary of performance using Apache Spark (Apache Parquet and FITS)\n",
    "\n",
    "Author: **Julien Peloton [@JulienPeloton](https://github.com/JulienPeloton)**  \n",
    "Last Run: **2018-10-19**  \n",
    "See also: [issue/249](https://github.com/LSSTDESC/DC2-production/issues/249)\n",
    "\n",
    "This notebook summarises the performance of data manipulations of the static coadd catalogs with Apache Spark.\n",
    "We focus on Parquet and FITS format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise our Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We focus on \"One Tract\" (OT) and \"All Tract\" (AT) catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be read from: \n",
      " /global/projecta/projectdirs/lsst/global/in2p3/Run1.1/summary\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/global/projecta/projectdirs/lsst/global/in2p3/Run1.1/summary'\n",
    "print(\"Data will be read from: \\n\", base_dir)\n",
    "\n",
    "# Path to data\n",
    "parq_4850_OT = os.path.join(base_dir, 'dpdd_object_tract_4850_hive.parquet')\n",
    "fits_4850_OT = os.path.join(base_dir, 'dpdd_object_tract_4850*.fits')\n",
    "\n",
    "parq_hive_AT = os.path.join(base_dir, 'dpdd_object.parquet')\n",
    "parq_simp_AT = os.path.join(base_dir, 'dpdd_object_simple.parquet')\n",
    "fits_simp_AT = os.path.join(base_dir, 'dpdd_object_tract_*.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here is a summary of the results. The configuration for this run was:\n",
    "\n",
    "- \"One Tract\" (OT) and \"All Tract\" (AT) catalogs\n",
    "- 1 full Cori compute node (32 cores).\n",
    "\n",
    "\n",
    "Numbers should be read as order of magnitude.\n",
    "Details can be found below.\n",
    "\n",
    "No cache:\n",
    "\n",
    "| Data set            | #Rows (size GB) | Load time | statistics* |\n",
    "|---------------------|-----------------|-------------|-----------------|\n",
    "| Parquet (OT)        | 719,228 (0.4)   | 192 ms ± 66.5 ms      |      189 ms ± 84 ms     |\n",
    "| FITS (OT)           | 719,228 (0.4)   | 3.74 s ± 107 ms      |      3.67 s ± 123 ms     |\n",
    "| Parquet (AT, Hive)  | 6,892,380 (4.5) | 726 ms ± 117 ms      |      990 ms ± 521 ms     |\n",
    "| Parquet (AT, Simple)| 6,892,380 (4.5) | 210 ms ± 28.2 ms      |      459 ms ± 46.5 ms     |\n",
    "| FITS (AT)           | 6,892,380 (4.5) | 25.7 s ± 308 ms      |      24.4 s ± 1.24 s     |\n",
    "\n",
    "_*statistics_ means computing: number of elements, mean, stddev, min, max.\n",
    "\n",
    "With cache (overhead of < 1 second to add to put data in cache):\n",
    "\n",
    "| Data set            | #Rows (size GB) | Load time | statistics |\n",
    "|---------------------|-----------------|-------------|-----------------|\n",
    "| Parquet (OT)        | 719,228 (0.4)   | 393 ms ± 86.2 ms      | 111 ms ± 45.4 ms |\n",
    "| FITS (OT)           | 719,228 (0.4)   | 312 ms ± 59.2 ms      | 99.5 ms ± 49.4 ms |\n",
    "| Parquet (AT, Hive)  | 6,892,380 (4.5) | 215 ms ± 102 ms      | 351 ms ± 150 ms\n",
    "| Parquet (AT, Simple)| 6,892,380 (4.5) | 181 ms ± 74.1 ms      | 391 ms ± 52.9 ms\n",
    "| FITS (AT)           | 6,892,380 (4.5) | 2.78 s ± 1.37 s      | 3.05 s ± 600 ms\n",
    "\n",
    "Remarks:\n",
    "\n",
    "- Results using Parquet are much faster than FITS. Reasons can be for e.g. columnar vs row-based or better implementation of the Spark Parquet connector. I believe this is related to what is seen with Dask between Parquet and HDF5.\n",
    "- For FITS, the number of input files matter. It is always better to have small number of large files rather than many small files.\n",
    "- Once data in cache, everything is super fast.\n",
    "- Note that given the small volume of data, most of the results below the seconds are basically dominated by Spark noise and not actual computation (which is why sometimes a simple count can be slower than computing full statistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "def readfits(path: str, hdu: int=1) -> DataFrame:\n",
    "    \"\"\" Wrapper around Spark reader for FITS\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the data. Can be a file, a folder, or a\n",
    "        glob pattern.\n",
    "    hdu : int, optional\n",
    "        HDU number to read. Default is 1.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame with the HDU data.\n",
    "\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"fits\").option(\"hdu\", hdu).load(path)\n",
    "\n",
    "def readparq(path: str) -> DataFrame:\n",
    "    \"\"\" Wrapper around Spark reader for Parquet\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the data. Can be a file, a folder, or a\n",
    "        glob pattern.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame with the HDU data.\n",
    "        \n",
    "    \"\"\"\n",
    "    return spark.read.format(\"parquet\").load(path)\n",
    "\n",
    "def simple_count(df: DataFrame, cache=False, txt: str=\"\") -> int:\n",
    "    \"\"\" Return the number of rows in the DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    cache : bool, optional\n",
    "        If True, put the Data in cache prior to the computation.\n",
    "        Data will be unpersisted afterwards. Default is False.\n",
    "    txt: str, optional\n",
    "        Additional text to be printed.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    out : int\n",
    "        Number of rows\n",
    "        \n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        start = time.time()\n",
    "        df = df.cache()\n",
    "        print(\"Cache took {:.1f} sec\".format(time.time() - start))\n",
    "    \n",
    "    res = df.count()\n",
    "    print(\"{} has length:\".format(txt), res)\n",
    "    \n",
    "    # Time it!\n",
    "    %timeit df.count()\n",
    "    \n",
    "    if cache:\n",
    "        df = df.unpersist()\n",
    "        \n",
    "    return res\n",
    "\n",
    "def stat_diff_col(\n",
    "        df: DataFrame, colname_1: str, colname_2: str, \n",
    "        cache=False, txt: str=\"\") -> DataFrame:\n",
    "    \"\"\" Return some statistics about the difference of \n",
    "    two DataFrame Columns.\n",
    "    Statistics include: count, mean, stddev, min, max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    colname_1 : str\n",
    "        Name of the first Column\n",
    "    colname_2 : str\n",
    "        Name of the second Column\n",
    "    cache : bool, optional\n",
    "        If True, put the Data in cache prior to the computation.\n",
    "        Data will be unpersisted afterwards. Default is False.\n",
    "    txt: str, optional\n",
    "        Additional text to be printed.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    out : DataFrame\n",
    "        DataFrame containing statistics about the Columns difference.\n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        df = df.cache()\n",
    "    print(\"{} has length:\".format(txt), df.count())\n",
    "    \n",
    "    # Time it!\n",
    "    %timeit res = df.select(col(colname_1) - col(colname_2)).describe().collect()\n",
    "    \n",
    "    if cache:\n",
    "        df = df.unpersist()\n",
    "    \n",
    "    return df.select(col(colname_1) - col(colname_2)).describe()\n",
    "\n",
    "\n",
    "def stat_one_col(df: DataFrame, colname: str, cache=False, txt: str=\"\") -> DataFrame:\n",
    "    \"\"\" Return some statistics about one DataFrame Column.\n",
    "    Statistics include: count, mean, stddev, min, max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    colname : str\n",
    "        Name of the Column for which we want the statistics\n",
    "    cache : bool, optional\n",
    "        If True, put the Data in cache prior to the computation.\n",
    "        Data will be unpersisted afterwards. Default is False.\n",
    "    txt: str, optional\n",
    "        Additional text to be printed.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    out : DataFrame\n",
    "        DataFrame containing statistics about the Column.\n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        df = df.cache()\n",
    "    print(\"{} has length:\".format(txt), df.count())\n",
    "    \n",
    "    # Time it!\n",
    "    %timeit res = df.select(colname).describe().collect()\n",
    "    \n",
    "    if cache:\n",
    "        df = df.unpersist()\n",
    "    \n",
    "    return df.select(colname).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OT (Parquet) has length: 719228\n",
      "192 ms ± 66.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "OT (FITS) has length: 719228\n",
      "3.74 s ± 107 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "AT (P-Hive) has length: 6892380\n",
      "726 ms ± 117 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "AT (P-simple) has length: 6892380\n",
      "210 ms ± 28.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "AT (FITS) has length: 6892380\n",
      "25.7 s ± 308 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Accessing catalogs\n",
    "cache = False\n",
    "df = readparq(parq_4850_OT)\n",
    "c = simple_count(df, cache, \"OT (Parquet)\")\n",
    "\n",
    "df = readfits(fits_4850_OT)\n",
    "c = simple_count(df, cache, \"OT (FITS)\")\n",
    "\n",
    "df = readparq(parq_hive_AT)\n",
    "c = simple_count(df, cache, \"AT (P-Hive)\")\n",
    "\n",
    "df = readparq(parq_simp_AT)\n",
    "c = simple_count(df, cache, \"AT (P-simple)\")\n",
    "\n",
    "df = readfits(fits_simp_AT)\n",
    "c = simple_count(df, cache, \"AT (FITS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OT (Parquet) has length: 719228\n",
      "189 ms ± 84 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "OT (FITS) has length: 719228\n",
      "3.67 s ± 123 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "AT (P-Hive) has length: 6892380\n",
      "990 ms ± 521 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "AT (P-simple) has length: 6892380\n",
      "459 ms ± 46.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "AT (FITS) has length: 6892380\n",
      "24.4 s ± 1.24 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Statistics: count, mean, stddev, min, max\n",
    "c1 = \"mag_g\"\n",
    "c2 = \"mag_r\"\n",
    "cache = False\n",
    "df = readparq(parq_4850_OT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"OT (Parquet)\")\n",
    "\n",
    "df = readfits(fits_4850_OT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"OT (FITS)\")\n",
    "\n",
    "df = readparq(parq_hive_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (P-Hive)\")\n",
    "\n",
    "df = readparq(parq_simp_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (P-simple)\")\n",
    "\n",
    "df = readfits(fits_simp_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (FITS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache took 0.5 sec\n",
      "OT (Parquet) has length: 719228\n",
      "393 ms ± 86.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Cache took 0.5 sec\n",
      "OT (FITS) has length: 719228\n",
      "312 ms ± 59.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Cache took 0.1 sec\n",
      "AT (P-Hive) has length: 6892380\n",
      "215 ms ± 102 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Cache took 0.0 sec\n",
      "AT (P-simple) has length: 6892380\n",
      "181 ms ± 74.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Cache took 0.5 sec\n",
      "AT (FITS) has length: 6892380\n",
      "The slowest run took 7.87 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "2.78 s ± 1.37 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[460] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing catalogs\n",
    "cache = True\n",
    "df = readparq(parq_4850_OT)\n",
    "c = simple_count(df, cache, \"OT (Parquet)\")\n",
    "df.rdd.unpersist()\n",
    "\n",
    "df = readfits(fits_4850_OT)\n",
    "c = simple_count(df, cache, \"OT (FITS)\")\n",
    "df.rdd.unpersist()\n",
    "\n",
    "df = readparq(parq_hive_AT)\n",
    "c = simple_count(df, cache, \"AT (P-Hive)\")\n",
    "df.rdd.unpersist()\n",
    "\n",
    "df = readparq(parq_simp_AT)\n",
    "c = simple_count(df, cache, \"AT (P-simple)\")\n",
    "df.rdd.unpersist()\n",
    "\n",
    "df = readfits(fits_simp_AT)\n",
    "c = simple_count(df, cache, \"AT (FITS)\")\n",
    "df.rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OT (Parquet) has length: 719228\n",
      "111 ms ± 45.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "OT (FITS) has length: 719228\n",
      "99.5 ms ± 49.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "AT (P-Hive) has length: 6892380\n",
      "351 ms ± 150 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "AT (P-simple) has length: 6892380\n",
      "391 ms ± 52.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "AT (FITS) has length: 6892380\n",
      "3.05 s ± 600 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1512] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistics: count, mean, stddev, min, max\n",
    "c1 = \"mag_g\"\n",
    "c2 = \"mag_r\"\n",
    "cache = True\n",
    "df = readparq(parq_4850_OT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"OT (Parquet)\")\n",
    "df.rdd.unpersist()\n",
    "\n",
    "df = readfits(fits_4850_OT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"OT (FITS)\")\n",
    "df.rdd.unpersist()\n",
    "\n",
    "df = readparq(parq_hive_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (P-Hive)\")\n",
    "df.rdd.unpersist()\n",
    "\n",
    "df = readparq(parq_simp_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (P-simple)\")\n",
    "df.rdd.unpersist()\n",
    "\n",
    "df = readfits(fits_simp_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (FITS)\")\n",
    "df.rdd.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_2.3.0_dev (2.3.0)",
   "language": "python",
   "name": "pyspark_2.3.0_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
