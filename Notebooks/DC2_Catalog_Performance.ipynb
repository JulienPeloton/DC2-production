{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tests of Coadd Catalog Access\n",
    "\n",
    "Test the performance of data manipulations of the static coadd catalogs.\n",
    "\n",
    "1. Identify trivial, moderate, and worst-case use case examples.\n",
    "2. Measure performance on\n",
    "    a. single patch\n",
    "    b. a single tract\n",
    "    c. the full dataset\n",
    "    3. Record data sizes of each of the above a, b, c.\n",
    "4. Determine if performance considerations mean we should generate a static file that contains a restricted set in columns.\n",
    "5. Look into again using full tables functionality to write HDF5 files so that they can be read by column efficiently. This was previously not possible because of an error trying to write the thousands of columns in our full coadd catalogs. This is #158\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import GCRCatalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the GCR reader outside of NERSC environment, you can override the `base_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "trim_config = config.copy()\n",
    "trim_config['filename_pattern'] = r'trim_merged_tract_\\d+\\.hdf5'\n",
    "table_trim_config = config.copy()\n",
    "table_trim_config['filename_pattern'] = r'table_trim_merged_tract_\\d+\\.hdf5'\n",
    "\n",
    "trim_onetract_config = config.copy()\n",
    "trim_onetract_config['filename_pattern'] = 'trim_merged_tract_4850.hdf5'\n",
    "table_trim_onetract_config = config.copy()\n",
    "table_trim_onetract_config['filename_pattern'] = 'table_trim_merged_tract_4850.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807 ms ± 20.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc_onetract = GCRCatalogs.load_catalog('dc2_coadd_run1.1p_tract4850', config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trim files are 1/10 of the size of the full files.  The `load_catalog` doesn't load the data, but does need to open and touch each file to read the metadata.  This is only about 4 times faster for the trim catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 ms ± 20.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc_onetract_trim = GCRCatalogs.load_catalog('dc2_coadd_run1.1p_tract4850', trim_onetract_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_onetract = GCRCatalogs.load_catalog('dc2_coadd_run1.1p_tract4850', config)\n",
    "gc_onetract_trim = GCRCatalogs.load_catalog('dc2_coadd_run1.1p_tract4850', trim_onetract_config)\n",
    "gc_onetract_table_trim = GCRCatalogs.load_catalog('dc2_coadd_run1.1p_tract4850', table_trim_onetract_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.92 s ± 140 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc = GCRCatalogs.load_catalog('dc2_coadd_run1.1p', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29 s ± 36.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc_trim = GCRCatalogs.load_catalog('dc2_coadd_run1.1p', trim_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = GCRCatalogs.load_catalog('dc2_coadd_run1.1p', config)\n",
    "gc_trim = GCRCatalogs.load_catalog('dc2_coadd_run1.1p', trim_config)\n",
    "gc_table_trim = GCRCatalogs.load_catalog('dc2_coadd_run1.1p', table_trim_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the GCR Catalog is, in principle, just the initialization of the catalog.  In practice the GCRCatalog reader does need to read through all of the metadata in the HDF5 files to figure out what's in there and available.  The onetract version is reading a 7.4 GB file that should fit in memory.  The full Run 1.1p is 78 GB, which does not fit in the average desktop memory.  This size could pontentially fit in the memory of various high-memory shared nodes.  This difference in size is conveniently roughly a factor of 10.  We should expect \n",
    "\n",
    "We can control the memory caching within GCR to clear the cache to reset for performance tests.  It's harder to control the underlying caching of the GPFS and kernel filesystem memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_color_slow(catalog):\n",
    "    \"\"\"Compute the mean g-r color of all objects in the 'catalog'.\n",
    "    \n",
    "    This is a trivial performance case.\n",
    "    This isn't particularly immediately interesting, but it's a simple arithmetic operation between two columns.\n",
    "    \"\"\"\n",
    "    average_gmr = np.mean(catalog['mag_g'] - catalog['mag_r'])\n",
    "    return average_gmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_color_faster(catalog):\n",
    "    \"\"\"Compute the mean g-r color of all objects in the 'catalog'.\n",
    "    \n",
    "    This is a trivial performance case.\n",
    "    This isn't particularly immediately interesting, but it's a simple arithmetic operation between two columns.\n",
    "    \"\"\"\n",
    "    data = catalog.get_quantities(['mag_g', 'mag_r'])\n",
    "    average_gmr = np.mean(data['mag_g'] - data['mag_r'])\n",
    "    return average_gmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_color_faster_iter(catalog):\n",
    "    \"\"\"Compute the mean g-r color of all objects in the 'catalog' using iterator.\n",
    "    \n",
    "    This is a trivial performance case.\n",
    "    This isn't particularly immediately interesting, but it's a simple arithmetic operation between two columns.\n",
    "    \"\"\"\n",
    "    sum_gmr = count = 0\n",
    "    for data in catalog.get_quantities(['mag_g', 'mag_r'], return_iterator=True):\n",
    "        sum_gmr += np.sum(data['mag_g'] - data['mag_r'])\n",
    "        count += len(data['mag_g'])\n",
    "    return sum_gmr / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_stellar_locus():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average color calculation is 5 times faster with the trim files for one tract, using the slowest most naive way to access the quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.66 s ± 985 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc_onetract.clear_cache()\n",
    "compute_mean_color_slow(gc_onetract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826 ms ± 40.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc_onetract_trim.clear_cache()\n",
    "compute_mean_color_slow(gc_onetract_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3min 6s ± 660 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc.clear_cache()\n",
    "compute_mean_color_slow(gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.31 s ± 1.11 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc_onetract.clear_cache()\n",
    "compute_mean_color_faster(gc_onetract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.64 s ± 91.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc_onetract.clear_cache()\n",
    "compute_mean_color_faster_iter(gc_onetract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3min 43s ± 43 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc.clear_cache()\n",
    "compute_mean_color_faster(gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3min 12s ± 1.58 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc.clear_cache()\n",
    "compute_mean_color_faster_iter(gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2 s ± 2.34 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gc_trim.clear_cache()\n",
    "compute_mean_color_faster(gc_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "gc_trim.clear_cache()\n",
    "compute_mean_color_faster_iter(gc_trim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to using Pandas to read the data files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "tract = 4850\n",
    "datafile_basename = 'merged_tract_%d.hdf5' % tract\n",
    "datafile_basename_trim = 'trim_' + datafile_basename\n",
    "datafile_basename_table_trim = 'table_trim_' + datafile_basename\n",
    "\n",
    "base_dir = gc_onetract_trim.base_dir\n",
    "\n",
    "datafile = os.path.join(base_dir, datafile_basename)\n",
    "datafile_trim = os.path.join(base_dir, datafile_basename_trim)\n",
    "datafile_table_trim = os.path.join(base_dir, datafile_basename_table_trim)\n",
    "\n",
    "key_prefix = 'coadd'\n",
    "nx, ny = 8, 8\n",
    "patches = ['%d%d' % (i, j) for i in range(nx) for j in range (ny)]  # Note '%d%d' instead of '%d,%d'\n",
    "patch = patches[0]\n",
    "key = '%s_%d_%s' % (key_prefix, tract, patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading just one patch of the tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.5 ms ± 1.74 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df = pd.read_hdf(datafile, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 ms ± 548 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_trim = pd.read_hdf(datafile_trim, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.8 ms ± 1.49 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_table_trim = pd.read_hdf(datafile_table_trim, key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load all of th patches in the tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tract_into_pandas(datafile, tract, key_prefix='coadd'):\n",
    "    nx, ny = 8, 8\n",
    "    patches = ['%d%d' % (i, j) for i in range(nx) for j in range (ny)]  # Note '%d%d' instead of '%d,%d'\n",
    "\n",
    "    dfs = []\n",
    "    for patch in patches:\n",
    "        key = '%s_%d_%s' % (key_prefix, tract, patch)\n",
    "        try:\n",
    "            df = pd.read_hdf(datafile, key=key)\n",
    "        except:\n",
    "            continue\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.82 s ± 94.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_trim = load_tract_into_pandas(datafile_trim, tract=tract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.59 s ± 1.51 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_table_trim = load_tract_into_pandas(datafile_table_trim, tract=tract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we use Dask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as da\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract = 4850\n",
    "\n",
    "base_dir = gc_onetract_trim.base_dir\n",
    "\n",
    "datafile = os.path.join(base_dir, 'table_trim_merged_tract_%d.hdf5' % tract)\n",
    "datafile_pattern = os.path.join(base_dir, 'table_trim_merged_tract_*.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_df = dd.read_hdf(datafile, key='coadd_*', mode='r')\n",
    "da_df_all = dd.read_hdf(datafile_pattern, key='coadd_*', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_hdf in module dask.dataframe.io.hdf:\n",
      "\n",
      "read_hdf(pattern, key, start=0, stop=None, columns=None, chunksize=1000000, sorted_index=False, lock=True, mode='a')\n",
      "    Read HDF files into a Dask DataFrame\n",
      "    \n",
      "    Read hdf files into a dask dataframe. This function is like\n",
      "    ``pandas.read_hdf``, except it can read from a single large file, or from\n",
      "    multiple files, or from multiple keys from the same file.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    pattern : string, list\n",
      "        File pattern (string), buffer to read from, or list of file\n",
      "        paths. Can contain wildcards.\n",
      "    key : group identifier in the store. Can contain wildcards\n",
      "    start : optional, integer (defaults to 0), row number to start at\n",
      "    stop : optional, integer (defaults to None, the last row), row number to\n",
      "        stop at\n",
      "    columns : list of columns, optional\n",
      "        A list of columns that if not None, will limit the return\n",
      "        columns (default is None)\n",
      "    chunksize : positive integer, optional\n",
      "        Maximal number of rows per partition (default is 1000000).\n",
      "    sorted_index : boolean, optional\n",
      "        Option to specify whether or not the input hdf files have a sorted\n",
      "        index (default is False).\n",
      "    lock : boolean, optional\n",
      "        Option to use a lock to prevent concurrency issues (default is True).\n",
      "    mode : {'a', 'r', 'r+'}, default 'a'. Mode to use when opening file(s).\n",
      "        'r'\n",
      "            Read-only; no data can be modified.\n",
      "        'a'\n",
      "            Append; an existing file is opened for reading and writing,\n",
      "            and if the file does not exist it is created.\n",
      "        'r+'\n",
      "            It is similar to 'a', but the file must already exist.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    dask.DataFrame\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Load single file\n",
      "    \n",
      "    >>> dd.read_hdf('myfile.1.hdf5', '/x')  # doctest: +SKIP\n",
      "    \n",
      "    Load multiple files\n",
      "    \n",
      "    >>> dd.read_hdf('myfile.*.hdf5', '/x')  # doctest: +SKIP\n",
      "    \n",
      "    >>> dd.read_hdf(['myfile.1.hdf5', 'myfile.2.hdf5'], '/x')  # doctest: +SKIP\n",
      "    \n",
      "    Load multiple datasets\n",
      "    \n",
      "    >>> dd.read_hdf('myfile.1.hdf5', '/*')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dd.read_hdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = np.mean(da_df['g_mag'] - da_df['r_mag'])\n",
    "df2_all = np.mean(da_df_all['g_mag'] - da_df_all['r_mag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.06 s ± 57.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.7 s ± 20.7 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df2_all.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the full DASK calculation takes 20 times longer than the onetract calculation for a data volume 10-times larger.\n",
    "\n",
    "DASK takes ~38 seconds to do the color average using the `table_trim_` file, while GCRCatalogs takes ~12 seconds using the `trim_` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv('OMP_NUM_THREADS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "    1. Put on SCRATCH (Lustre)\n",
    "    2. Put on ??? (burst buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/global/cscratch1/sd/wmwv/DC2/Run1.1p/summary'\n",
    "\n",
    "datafile_lustre = os.path.join(base_dir, 'table_trim_merged_tract_%d.hdf5' % tract)\n",
    "datafile_pattern_lustre = os.path.join(base_dir, 'table_trim_merged_tract_*.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_df_lustre = dd.read_hdf(datafile_lustre, key='coadd_*', mode='r')\n",
    "da_df_all_lustre = dd.read_hdf(datafile_pattern_lustre, key='coadd_*', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_lustre = np.mean(da_df_lustre['g_mag'] - da_df_lustre['r_mag'])\n",
    "df2_all_lustre = np.mean(da_df_all_lustre['g_mag'] - da_df_all_lustre['r_mag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.02 s ± 86.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df2_lustre.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.4 s ± 23.2 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df2_all_lustre.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time on the Luster file system (2, 42) sec seems about the same as the GPFS (2, 50) sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'array': {'chunk-size': '128MiB', 'rechunk-threshold': 4}, 'scheduler': 'threads'}\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "dask.config.set(scheduler='threads')\n",
    "print(dask.config.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.26 s ± 599 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df2_lustre.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python-dask",
   "language": "python",
   "name": "desc-python-dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
