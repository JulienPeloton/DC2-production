{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![](./header.png) -->\n",
    "<img src=\"./header.png\",width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The DC2 Cookbook: Recipes for Emulating the LSST DM Data Release Processing Pipeline\n",
    "\n",
    "*Simon Krughoff, Phil Marshall and others*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this Note we describe the sequence of LSST data management (DM) software stack calls needed to emulate the expected Data Release Processing (DRP) pipeline. These recipes were initially developed for the Twinkles project, the pathfinder for the first LSST DESC data challenge, DC1, and were later adapted for the main DC1 simulation. We provide links to the original DC1 recipes at the appropriate points below. \n",
    "\n",
    "The target DRP pipeline is described in the LSST Project document [LDM-151](ls.st/ldm-151), _\"LSST Data Management Science Pipelines Design.\"_ We will refer to this document extensively, importing figures and quoting from it as needed. In DC1, the simulated images that we produced were so-called \"e-images\", ecah one of which emulates a calibrated frame; in DC2, we will produce amplifier (science) and accompanying calibration images, and extend the Twinkles pipeline to include instrument signature removal. We also adopt the recently developed object association code, in order to make `DIAObjects` from our `DIASources`. Other than these two extensions, we expect the DC2 DM pipeline to look similar to the DC1 pipeline. \n",
    "\n",
    "<a id='toc'></a>\n",
    "This Note is organized as follows. We first provide an [overview](#overview) of the DRP pipeline, summarizing the relevant section in [LDM-151](ls.st/ldm-151). Then, we present the recipes in three sections: \n",
    "1. [Image Coaddition and Object Detection](#coadds)\n",
    "2. [Difference Image Analysis and DIASource Detection](#diasources)\n",
    "3. [DIAObject Generation and Light Curve Forced Photometry](#forcedphot)\n",
    "We then provide some brief [concluding remarks](#conclusions).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overview'></a>\n",
    "## Data Release Processing Overview\n",
    "\n",
    "The DRP pipelines are summarized in [LDM-151](ls.st/ldm-151) by the following Figure:\n",
    "\n",
    "<img src=\"https://github.com/lsst/LDM-151/raw/master/figures/drp_summary.png\" width=40% align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anticipating that difference image analysis (DIA) will not be required in the DC2 main survey area, it makes sense to preserve the DC1 grouping of the DRP pipelines, into static sky and dynamic sky pipelines. \n",
    "  * The static sky analysis involves image characterization, calibration and coaddition followed by `Object` generation and measurement. \n",
    "  * The dynamic sky analysis makes use of the calibrated images, but involves 1) the construction of a `TemplateCoadd` image followed by image differencing, `DIASource` detection in the difference images, and then 2) association of those `DIASources` into `DIAObjects` which then form the target positions for forced photometry. Light curves can then be extracted from the `DIAForcedSource` table.\n",
    "\n",
    "At the time of writing, multi-epoch object characterization is not yet available in the DM stack. As a result, the `Objects` are the same as the \"Preliminary Objects\", and are made by detecting sources in the `Coadd` images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the table of contents.](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='coadds'></a>\n",
    "## Image Coaddition and `Object` Detection\n",
    "\n",
    "This recipe started life as the original Twinkles Cookbook recipe, [_\"Recipe: Emulating the DM Level 2 Pipeline\"_](https://github.com/LSSTDESC/Twinkles/blob/master/doc/Cookbook/DM_Level2_Recipe.md). This was intended to show how to process simulated image data as if we were running the static sky part of the LSST DM DRP. The primary products are catalogs of `Objects.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the indexes for astrometric and photometric calibration\n",
    "\n",
    "We use the `phoSim` reference catalogs to emulate the kind of high accuracy\n",
    "calibration that we expect to be possible with the LSST data. This is an\n",
    "approximation, but for many purposes a good one. \n",
    "\n",
    "Currently the reference catalogs need to be formatted as astrometry.net index files.  I can convert the\n",
    "reference catalog produced by `generatePhosimInput.py`, but there are a couple of precursor steps.  First,\n",
    "there is a bug in how phosim creates the nominal WCS (PHOSIM-18).  The result is that the WCS claims to be\n",
    "ICRS but ignores precession.  Since the matching algorithms assume we know approximately where the telescope\n",
    "is pointing, they fail unless the catalogs are fixed.\n",
    "\n",
    "It is easier to hack the reference catalog than to fix every WCS in every image, so I just correct for the approximate\n",
    "precession and it gets close enough that the matching algorithms fail (the WCS will still be wrong, but we don't really\n",
    "care since we aren't comparing to external catalogs).\n",
    "\n",
    "```bash\n",
    "$> awk '{printf(\"%i, %f, %f, %f, %f, %f, %i, %i\\n\", $1, $2-0.0608766, $3-0.0220287, $4, $5,$6,$7,$8)}' twinkles_ref.txt >twinkles_ref_obs.txt\n",
    "```\n",
    "The first few lines look like this:\n",
    "```\n",
    "#uniqueId, raJ2000, decJ2000, lsst_g, lsst_r, lsst_i, starnotgal, isvariable\n",
    "992887068676, 52.989609, -27.381822, 26.000570, 24.490695, 22.338254, 1, 0\n",
    "1605702564868, 53.002656, -27.356515, 27.732406, 26.371370, 25.372229, 1, 0\n",
    "1277139994628, 52.991627, -27.362006, 24.948391, 23.598418, 22.391097, 1, 0\n",
    "1704223204356, 53.017637, -27.326836, 23.914298, 22.938313, 22.539221, 1, 0\n",
    "1605697082372, 53.017005, -27.333503, 21.839375, 21.498586, 21.378259, 1, 0\n",
    "1605694183428, 52.988539, -27.326388, 25.324673, 24.003677, 23.221476, 1, 0\n",
    "1605694345220, 52.992405, -27.326471, 19.366450, 18.940676, 18.774756, 1, 0\n",
    "1277138139140, 52.994290, -27.333325, 24.185304, 22.843333, 21.513559, 1, 0\n",
    "1605701058564, 53.008024, -27.350062, 21.925079, 21.523769, 21.378805, 1, 0\n",
    "```\n",
    "Now we translate the text file into a FITS file for indexing. I decided to change the column names from the default output by CatSim.\n",
    "Then you can do the actual index generation.  You'll need to set up a couple of packages then run some scripts to do the formatting.\n",
    "```bash\n",
    "$> setup astrometry_net\n",
    "$> setup pyfits\n",
    "$> text2fits.py -H 'id, ra, dec, g, r, i, starnotgal, isvariable' -s ', ' twinkles_ref_obs.txt twinkles_ref.fits -f 'kdddddjj'\n",
    "$> export P=0106160\n",
    "$> build-astrometry-index -i twinkles_ref.fits -o index-${P}00.fits -I ${P}00 -P 0 -S r -n 100 -L 20 -E -j 0.4 -r 1 > build-00.log\n",
    "$> build-astrometry-index -1 index-${P}00.fits -o index-${P}01.fits -I ${P}01 -P 1 -S r -L 20 -E -M -j 0.4 > build-01.log &\n",
    "$> build-astrometry-index -1 index-${P}00.fits -o index-${P}02.fits -I ${P}02 -P 2 -S r -L 20 -E -M -j 0.4 > build-02.log &\n",
    "$> build-astrometry-index -1 index-${P}00.fits -o index-${P}03.fits -I ${P}03 -P 3 -S r -L 20 -E -M -j 0.4 > build-03.log &\n",
    "$> build-astrometry-index -1 index-${P}00.fits -o index-${P}04.fits -I ${P}04 -P 4 -S r -L 20 -E -M -j 0.4 > build-04.log\n",
    "$> mkdir and_files\n",
    "$> mv index*.fits and_files\n",
    "$> cd and_files\n",
    "```\n",
    "The matcher needs to know which index files are available and what columns to use for photometric calibration.  These are specified using a configuration file.  This file goes in the `and_files` directory.  It is called `andConfig.py` and looks like this:\n",
    "```\n",
    "root.starGalaxyColumn = \"starnotgal\"\n",
    "root.variableColumn = \"isvariable\"\n",
    "filters = ('u', 'g', 'r', 'i', 'z', 'y')\n",
    "root.magColumnMap = {'u':'g', 'g':'g', 'r':'r', 'i':'i', 'z':'i', 'y':'i'}\n",
    "root.indexFiles = ['index-010616000.fits',\n",
    "'index-010616001.fits',\n",
    "'index-010616002.fits',\n",
    "'index-010616003.fits',\n",
    "'index-010616004.fits']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the data to run DM processing\n",
    "\n",
    "First you'll need to build the stack using tickets/DM-4302 of obs_lsstSim.  In order to patch a branch version onto a pre-existing stack you can do something like the following.\n",
    "\n",
    "1. Build a master stack.  I suggest using [lsstsw](https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool).\n",
    "2. Set up the stack: e.g. `$> setup obs_lsstSim -t bNNNN`\n",
    "3. Clone the package you want to patch on top of your stack `$> clone git@github.com:lsst/obs_lsstSim.git; cd obs_lsstSim`\n",
    "4. Get the branch: `$> checkout tickets/DM-4302`\n",
    "5. Set up just (-j) the cloned package (since the rest of the packages are already set up): `$> setup -j -r .`\n",
    "6. Build the cloned package (this is necessary even for pure python packages): `$> scons opt=3`\n",
    "7. Optionally install it in your stack: `$> scons install declare`\n",
    "\n",
    "This assumes the simulated images have landed in a directory called ```images```\n",
    "in the current directory.  In the images directory, you'll need a ```_mapper``` file with contents\n",
    "```python\n",
    "lsst.obs.lsstSim.LsstSimMapper\n",
    "```\n",
    "The above file will tell the stack where to put the raw files and eimages.\n",
    "```bash\n",
    "# Setup the stack environment.  This will make the LsstSimMapper class available\n",
    "$> setup obs_lsstSim\n",
    "\n",
    "# Ingest the images from a directory called images to a repository called input_data\n",
    "# there are some config overrides in the ingest.py file\n",
    "$> ingestImages.py images images/lsst_*.fits.gz --mode link --output input_data\n",
    "```\n",
    "Now you are setup to process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the image data using the DM stack\n",
    "\n",
    "Start here if you just want to exercise the DM stack.  If you didn't follow the steps above, first get the data and astrometry.net index files from\n",
    "[here](https://lsst-web.ncsa.illinois.edu/~krughoff/data/gri_data.tar.gz).  Then untar the tarball in a working directory.\n",
    "\n",
    "After you have the data, you can start following the steps below to get, for example, forced photometry in three bands. First, set up the reference catalog for photometric and astrometric calibration:\n",
    "```bash\n",
    "$> setup -m none -r and_files astrometry_net_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create calibrated images from the input eimages.  This will write to a repository called output_data.  The --id argument defines the data to operate on.  In this case it means process all data (in this example the g, r, and i bands) with visit numbers between 840 and 879.  Missing data will be skipped.\n",
    "```bash\n",
    "$> processEimage.py input_data/ --id visit=840..879 --output output_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a skyMap to use as the basis for the astrometic system for the coadds.  This can't be done up front because\n",
    "makeDiscreteSkyMap decides how to build the patches and tracts for the skyMap based on the data.\n",
    "```bash\n",
    "$> makeDiscreteSkyMap.py output_data/ --id visit=840..879 --output output_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coadds are done in two steps.  Step one is to warp the data to a common astrometric system.  The following does that.\n",
    "The config option is to use background subtracted exposures as inputs.  You can also specify visits using the ^ operator meaning 'and'.\n",
    "```bash\n",
    "$> makeCoaddTempExp.py output_data/ --selectId visit=840..849 --id filter=r patch=0,0 tract=0 --config bgSubtracted=True --output output_data\n",
    "$> makeCoaddTempExp.py output_data/ --selectId visit=860..869 --id filter=g patch=0,0 tract=0 --config bgSubtracted=True --output output_data\n",
    "$> makeCoaddTempExp.py output_data/ --selectId visit=870..879 --id filter=i patch=0,0 tract=0 --config bgSubtracted=True --output output_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second step which actually coadds the warped images.  The doInterp config option is required if there\n",
    "are any NaNs in the image (which there will be for this set since the images do not cover the whole patch).\n",
    "```bash\n",
    "$> assembleCoadd.py output_data/ --selectId visit=840..849 --id filter=r patch=0,0 tract=0 --config doInterp=True --output output_data\n",
    "$> assembleCoadd.py output_data/ --selectId visit=860..869 --id filter=g patch=0,0 tract=0 --config doInterp=True --output output_data\n",
    "$> assembleCoadd.py output_data/ --selectId visit=870..879 --id filter=i patch=0,0 tract=0 --config doInterp=True --output output_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect sources in the coadd and then merge detections from multiple bands.\n",
    "```bash\n",
    "$> detectCoaddSources.py output_data/ --id tract=0 patch=0,0 filter=g^r^i --output output_data\n",
    "$> mergeCoaddDetections.py output_data/ --id tract=0 patch=0,0 filter=g^r^i --output output_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do measurement on the sources detected in the above steps and merge the measurements from multiple bands.\n",
    "```bash\n",
    "$> measureCoaddSources.py output_data/ --id tract=0 patch=0,0 filter=g^r^i --config measurement.doApplyApCorr=yes --output output_data\n",
    "$> mergeCoaddMeasurements.py output_data/ --id tract=0 patch=0,0 filter=g^r^i --output output_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the detections from the coadd to do forced photometry on all the single frame data.\n",
    "```bash\n",
    "$> forcedPhotCcd.py output_data/ --id tract=0 visit=840..879 sensor=1,1 raft=2,2 --config measurement.doApplyApCorr=yes --output output_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final step is not really necessary: it results in a `ForcedSource` table whose utility is questionable. We expect the light curves of supernovae to come from the forced photometry of the `DIASources` (see below). However, the forced photometry of the static sky `Objects` may provide some useful comparisons, so we include it.\n",
    "\n",
    "\n",
    "<!-- Closing remarks from the Twinkles recipe:\n",
    "\n",
    "Once the forced photometry is done, you can look at the output by loading the measurements using the butler.  [This script](../../bin/plot_point_mags.py) shows how to start looking at the measurements.  It produces the following image.  I tried to fit both the systematic floor and the 5-sigma value for each of the bands.  Results are shown in the legend of the following image.\n",
    "\n",
    "![Repeat figure](gri_err.png)\n",
    "\n",
    "You can also use the stack to make a color image from the three coadds.  See [colorim.py](../../bin/colorim.py) for the code to do this.  Note that you can also overplot the detections.\n",
    "\n",
    "[![Coadd thumbnail](rgb_coadd_thumb.png)](rgb_coadd.png)\n",
    "\n",
    "-->\n",
    "\n",
    "[Back to the table of contents.](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='diasources'></a>\n",
    "## Difference Image Analysis and `DIASource` Detection\n",
    "\n",
    "[Back to the table of contents.](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='forcedphot'></a>\n",
    "## `DIAObject` Generation and Light Curve Forced Photometry\n",
    "\n",
    "[Back to the table of contents.](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Concluding Remarks\n",
    "\n",
    "[Back to the table of contents.](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables\n",
    "\n",
    "Tables can be fiddly in `Markdown`. A good place to start is an online table generator like [this one](http://www.tablesgenerator.com/markdown_tables). Then, you'll need some patience. For more on table formatting, see the [`Markdown` cheat-sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables).\n",
    "\n",
    "|   A   |   B   |      C         |  D  |\n",
    "|:-----:|:-----:|:--------------:|:---:|\n",
    "| (deg) | (kpc) | ($M_{\\odot}$)  |     |\n",
    "|  0.4  |  3.4  |  $10^{12.2}$   | R,S |\n",
    "|  9.6  |  8.2  |  $10^{10.4}$   |  S  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "For a guide to writing `Markdown` documents, check out this [useful little cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
